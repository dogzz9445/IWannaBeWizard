{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# https://www.kaggle.com/amikhshibu/shape-recognition-with-inceptionv3-acc-95\r\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\r\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input, decode_predictions\r\n",
    "from tensorflow.keras.preprocessing import image\r\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras.layers import Input\r\n",
    "from tensorflow.keras.layers import Conv2D\r\n",
    "from tensorflow.keras.layers import BatchNormalization\r\n",
    "from tensorflow.keras.layers import MaxPooling2D\r\n",
    "from tensorflow.keras.layers import Flatten\r\n",
    "from tensorflow.keras.layers import Dropout\r\n",
    "from tensorflow.keras.layers import Dense\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping\r\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\r\n",
    "import numpy as np\r\n",
    "from sklearn.metrics import classification_report"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-02-01 13:26:47.449621: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-01 13:26:47.449664: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    " # Split dataset into Train, Validation & Test sets\r\n",
    "\r\n",
    "parent_folder = './train_data'\r\n",
    "size = 299   # Resize all images to (size,size)\r\n",
    "bs = 32      # Batch size\r\n",
    "\r\n",
    "\r\n",
    "# Data augmentation on train dataset only\r\n",
    "train_data_gen = ImageDataGenerator(width_shift_range = 0.1, height_shift_range = 0.1, zoom_range=0.1, shear_range=0.1, brightness_range=[0.8,1.2], validation_split=0.15, preprocessing_function=preprocess_input)\r\n",
    "train_data = train_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', batch_size=bs, seed=42, subset='training')\r\n",
    "\r\n",
    "validation_data_gen = ImageDataGenerator(validation_split=0.15, preprocessing_function=preprocess_input)\r\n",
    "validation_data = validation_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', batch_size=bs, seed=42, subset='validation')\r\n",
    "\r\n",
    "test_data_gen = ImageDataGenerator(validation_split=0.10, preprocessing_function=preprocess_input)\r\n",
    "test_data = test_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', subset='validation', shuffle=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 357 images belonging to 4 classes.\n",
      "Found 63 images belonging to 4 classes.\n",
      "Found 42 images belonging to 4 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Assign essential variables\r\n",
    "shape = train_data.image_shape                 # Shape of train images (height,width,channels)\r\n",
    "print(shape)\r\n",
    "k = train_data.num_classes                     # Total number of labels or classes\r\n",
    "train_samples = train_data.samples             # Total number of images in train set\r\n",
    "validation_samples = validation_data.samples   # total number of images in validation set"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(299, 299, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Build the model\r\n",
    "\r\n",
    "input = Input(shape=shape)\r\n",
    " \r\n",
    "basemodel = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=shape, pooling='avg')   # Basemodel is InceptionV3 with pretrained weights trained on imagenet dataset\r\n",
    "basemodel.trainable = False                                                                        # Freeze the weights in all layers of the CNN\r\n",
    " \r\n",
    "x = basemodel(input)\r\n",
    "x = Dense(1024, activation='relu')(x)\r\n",
    "x = Dropout(0.2)(x)\r\n",
    "output = Dense(k, activation='softmax')(x)\r\n",
    " \r\n",
    "model = Model(input,output)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 4s 0us/step\n",
      "219070464/219055592 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Compile the model\r\n",
    "\r\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Initialize callbacks\r\n",
    " \r\n",
    "stop = EarlyStopping(monitor='val_loss', patience=4, mode='min', restore_best_weights=True)                                             # Stops training early to prevent overfitting\r\n",
    "checkpoint = ModelCheckpoint(filepath='./weights/{val_loss:.4f}-weights-{epoch:02d}.hdf5', monitor='val_loss', mode='min', save_best_only=True)   # Saves the model for every best val_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Train the model\r\n",
    "\r\n",
    "ep = 50                      # Number of epochs\r\n",
    "spe = train_samples/bs       # Steps per epoch\r\n",
    "vs = validation_samples/bs   # Validation steps\r\n",
    " \r\n",
    "r = model.fit(train_data, validation_data=validation_data, steps_per_epoch=spe, validation_steps=vs, epochs=ep, callbacks=[stop,checkpoint])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 33s 2s/step - loss: 0.4205 - accuracy: 0.8207 - val_loss: 0.1033 - val_accuracy: 0.9524\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 25s 2s/step - loss: 0.0437 - accuracy: 0.9860 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.0164 - accuracy: 0.9972 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 27s 2s/step - loss: 0.0163 - accuracy: 0.9944 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 27s 2s/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 27s 2s/step - loss: 0.0096 - accuracy: 0.9944 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.0104 - accuracy: 0.9944 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.0067 - accuracy: 0.9972 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 32s 3s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 34s 3s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 34s 3s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 34s 3s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 32s 3s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 32s 3s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 34s 3s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 36s 3s/step - loss: 9.0815e-04 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 33s 3s/step - loss: 5.4949e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 32s 3s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 34s 3s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 35s 3s/step - loss: 8.1758e-04 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 34s 3s/step - loss: 9.0751e-04 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 38s 3s/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 39s 3s/step - loss: 5.9107e-04 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 40s 4s/step - loss: 4.2606e-04 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Predictions on the test data\r\n",
    "\r\n",
    "pred = model.predict(test_data).argmax(axis=1)\r\n",
    "labels = list(train_data.class_indices.keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Get the F1 score on test data prediction\r\n",
    "\r\n",
    "print(classification_report(test_data.classes, pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00        12\n",
      "           3       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model.save('model/model_classification_20220116')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model/model_classification_20220116\\assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "img_path = 'triangle.png'\r\n",
    "img = image.load_img(img_path, target_size=(299, 299))\r\n",
    "x = image.img_to_array(img) \r\n",
    "x = np.expand_dims(x, axis=0)\r\n",
    "x = preprocess_input(x)\r\n",
    "\r\n",
    "preds = model.predict(x).argmax(axis=1)\r\n",
    "# decode the results into a list of tuples (class, description, probability)\r\n",
    "# (one such list for each sample in the batch)\r\n",
    "print('Predicted:', preds)\r\n",
    "# print(classification_report(test_data.classes, pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted: [3]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "from tensorflow_docs.vis import embed\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "import imageio"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "batch_size = 64\r\n",
    "num_channels = 1\r\n",
    "num_classes = 10\r\n",
    "image_size = 28\r\n",
    "latent_dim = 128"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# We'll use all the available examples from both the training and test\r\n",
    "# sets.\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n",
    "all_digits = np.concatenate([x_train, x_test])\r\n",
    "all_labels = np.concatenate([y_train, y_test])\r\n",
    "\r\n",
    "# Scale the pixel values to [0, 1] range, add a channel dimension to\r\n",
    "# the images, and one-hot encode the labels.\r\n",
    "all_digits = all_digits.astype(\"float32\") / 255.0\r\n",
    "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\r\n",
    "all_labels = keras.utils.to_categorical(all_labels, 10)\r\n",
    "\r\n",
    "# Create tf.data.Dataset.\r\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_digits, all_labels))\r\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\r\n",
    "\r\n",
    "print(f\"Shape of training images: {all_digits.shape}\")\r\n",
    "print(f\"Shape of training labels: {all_labels.shape}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n",
      "11501568/11490434 [==============================] - 2s 0us/step\n",
      "Shape of training images: (70000, 28, 28, 1)\n",
      "Shape of training labels: (70000, 10)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "generator_in_channels = latent_dim + num_classes\r\n",
    "discriminator_in_channels = num_channels + num_classes\r\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "138 11\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Create the discriminator.\r\n",
    "discriminator = keras.Sequential(\r\n",
    "    [\r\n",
    "        keras.layers.InputLayer((28, 28, discriminator_in_channels)),\r\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\r\n",
    "        layers.LeakyReLU(alpha=0.2),\r\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\r\n",
    "        layers.LeakyReLU(alpha=0.2),\r\n",
    "        layers.GlobalMaxPooling2D(),\r\n",
    "        layers.Dense(1),\r\n",
    "    ],\r\n",
    "    name=\"discriminator\",\r\n",
    ")\r\n",
    "\r\n",
    "# Create the generator.\r\n",
    "generator = keras.Sequential(\r\n",
    "    [\r\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\r\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\r\n",
    "        # 7x7x(128 + num_classes) map.\r\n",
    "        layers.Dense(7 * 7 * generator_in_channels),\r\n",
    "        layers.LeakyReLU(alpha=0.2),\r\n",
    "        layers.Reshape((7, 7, generator_in_channels)),\r\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\r\n",
    "        layers.LeakyReLU(alpha=0.2),\r\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\r\n",
    "        layers.LeakyReLU(alpha=0.2),\r\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\r\n",
    "    ],\r\n",
    "    name=\"generator\",\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Image Converters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "from tensorflow.keras.preprocessing import image as KerasImage\r\n",
    "from PIL import Image\r\n",
    "import io\r\n",
    "import numpy as np\r\n",
    "import base64"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "def resize_image_from_bytes(pre_image, original_size: tuple, target_size: tuple):\r\n",
    "    img = np.frombuffer(pre_image, dtype=np.byte).reshape(original_size + (3,))\r\n",
    "    img = KerasImage.array_to_img(img)\r\n",
    "    return img.resize(target_size)\r\n",
    "\r\n",
    "def resize_image_from_png(pre_image, target_size: tuple):\r\n",
    "    img = Image.open(io.BytesIO(pre_image))\r\n",
    "    return img.resize(target_size)\r\n",
    "\r\n",
    "def load_image(filename: str):\r\n",
    "    img = KerasImage.load_img(filename, target_size=(299, 299))\r\n",
    "    arr_img = KerasImage.img_to_array(img)\r\n",
    "    return arr_img\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "#\r\n",
    "circle_rgb = load_image('test_data/circle.png')\r\n",
    "dtype = np.dtype('B')\r\n",
    "circle_png = None\r\n",
    "with open('test_data/circle.png', 'rb') as f:\r\n",
    "    circle_png = np.fromfile(f, np.byte)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "img = resize_image_from_png(circle_png, (299, 299))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "if img.size == (299, 299):\r\n",
    "    print('true')\r\n",
    "else:\r\n",
    "    print('false')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "true\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "from tensorflow import keras\r\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "model = keras.models.load_model('model/model_classification.model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "#model.predict(preprocess_input(np.expand_dims(img, axis=0)))\r\n",
    "model.predict(preprocess_input(np.expand_dims(load_image('test_data/circle.png'), axis=0)))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "array() got an unexpected keyword argument 'axis'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9048/2198423329.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#model.predict(preprocess_input(np.expand_dims(img, axis=0)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_data/circle.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: array() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GAN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "class ConditionalGAN(keras.Model):\r\n",
    "    def __init__(self, discriminator, generator, latent_dim):\r\n",
    "        super(ConditionalGAN, self).__init__()\r\n",
    "        self.discriminator = discriminator\r\n",
    "        self.generator = generator\r\n",
    "        self.latent_dim = latent_dim\r\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\r\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\r\n",
    "\r\n",
    "    @property\r\n",
    "    def metrics(self):\r\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\r\n",
    "\r\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\r\n",
    "        super(ConditionalGAN, self).compile()\r\n",
    "        self.d_optimizer = d_optimizer\r\n",
    "        self.g_optimizer = g_optimizer\r\n",
    "        self.loss_fn = loss_fn\r\n",
    "\r\n",
    "    def train_step(self, data):\r\n",
    "        # Unpack the data.\r\n",
    "        real_images, one_hot_labels = data\r\n",
    "\r\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\r\n",
    "        # the images. This is for the discriminator.\r\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\r\n",
    "        image_one_hot_labels = tf.repeat(\r\n",
    "            image_one_hot_labels, repeats=[image_size * image_size]\r\n",
    "        )\r\n",
    "        image_one_hot_labels = tf.reshape(\r\n",
    "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\r\n",
    "        )\r\n",
    "\r\n",
    "        # Sample random points in the latent space and concatenate the labels.\r\n",
    "        # This is for the generator.\r\n",
    "        batch_size = tf.shape(real_images)[0]\r\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\r\n",
    "        random_vector_labels = tf.concat(\r\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\r\n",
    "        )\r\n",
    "\r\n",
    "        # Decode the noise (guided by labels) to fake images.\r\n",
    "        generated_images = self.generator(random_vector_labels)\r\n",
    "\r\n",
    "        # Combine them with real images. Note that we are concatenating the labels\r\n",
    "        # with these images here.\r\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\r\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\r\n",
    "        combined_images = tf.concat(\r\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\r\n",
    "        )\r\n",
    "\r\n",
    "        # Assemble labels discriminating real from fake images.\r\n",
    "        labels = tf.concat(\r\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\r\n",
    "        )\r\n",
    "\r\n",
    "        # Train the discriminator.\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            predictions = self.discriminator(combined_images)\r\n",
    "            d_loss = self.loss_fn(labels, predictions)\r\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\r\n",
    "        self.d_optimizer.apply_gradients(\r\n",
    "            zip(grads, self.discriminator.trainable_weights)\r\n",
    "        )\r\n",
    "\r\n",
    "        # Sample random points in the latent space.\r\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\r\n",
    "        random_vector_labels = tf.concat(\r\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\r\n",
    "        )\r\n",
    "\r\n",
    "        # Assemble labels that say \"all real images\".\r\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\r\n",
    "\r\n",
    "        # Train the generator (note that we should *not* update the weights\r\n",
    "        # of the discriminator)!\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            fake_images = self.generator(random_vector_labels)\r\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\r\n",
    "            predictions = self.discriminator(fake_image_and_labels)\r\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\r\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\r\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\r\n",
    "\r\n",
    "        # Monitor loss.\r\n",
    "        self.gen_loss_tracker.update_state(g_loss)\r\n",
    "        self.disc_loss_tracker.update_state(d_loss)\r\n",
    "        return {\r\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\r\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\r\n",
    "        }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "cond_gan.fit(dataset, epochs=20)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'discriminator' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9048/662719055.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m cond_gan = ConditionalGAN(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdiscriminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m cond_gan.compile(\n\u001b[0;32m      5\u001b[0m     \u001b[0md_optimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0003\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'discriminator' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We first extract the trained generator from our Conditiona GAN.\n",
    "trained_gen = cond_gan.generator\n",
    "\n",
    "# Choose the number of intermediate images that would be generated in\n",
    "# between the interpolation + 2 (start and last images).\n",
    "num_interpolation = 9  # @param {type:\"integer\"}\n",
    "\n",
    "# Sample noise for the interpolation.\n",
    "interpolation_noise = tf.random.normal(shape=(1, latent_dim))\n",
    "interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)\n",
    "interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
    "\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    # Convert the start and end labels to one-hot encoded vectors.\n",
    "    first_label = keras.utils.to_categorical([first_number], num_classes)\n",
    "    second_label = keras.utils.to_categorical([second_number], num_classes)\n",
    "    first_label = tf.cast(first_label, tf.float32)\n",
    "    second_label = tf.cast(second_label, tf.float32)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels.\n",
    "    percent_second_label = tf.linspace(0, 1, num_interpolation)[:, None]\n",
    "    percent_second_label = tf.cast(percent_second_label, tf.float32)\n",
    "    interpolation_labels = (\n",
    "        first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "    )\n",
    "\n",
    "    # Combine the noise and the labels and run inference with the generator.\n",
    "    noise_and_labels = tf.concat([interpolation_noise, interpolation_labels], 1)\n",
    "    fake = trained_gen.predict(noise_and_labels)\n",
    "    return fake\n",
    "\n",
    "\n",
    "start_class = 1  # @param {type:\"slider\", min:0, max:9, step:1}\n",
    "end_class = 5  # @param {type:\"slider\", min:0, max:9, step:1}\n",
    "\n",
    "fake_images = interpolate_class(start_class, end_class)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fake_images *= 255.0\n",
    "converted_images = fake_images.astype(np.uint8)\n",
    "converted_images = tf.image.resize(converted_images, (96, 96)).numpy().astype(np.uint8)\n",
    "imageio.mimsave(\"animation.gif\", converted_images, fps=1)\n",
    "embed.embed_file(\"animation.gif\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa191c739e4238b6bce09f0aa2a4d176429cce0a8f0c895412859a226c16b236"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.9 64-bit ('venv')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}