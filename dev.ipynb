{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# https://www.kaggle.com/amikhshibu/shape-recognition-with-inceptionv3-acc-95\r\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\r\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input, decode_predictions\r\n",
    "from tensorflow.keras.preprocessing import image\r\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras.layers import Input\r\n",
    "from tensorflow.keras.layers import Conv2D\r\n",
    "from tensorflow.keras.layers import BatchNormalization\r\n",
    "from tensorflow.keras.layers import MaxPooling2D\r\n",
    "from tensorflow.keras.layers import Flatten\r\n",
    "from tensorflow.keras.layers import Dropout\r\n",
    "from tensorflow.keras.layers import Dense\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping\r\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\r\n",
    "import numpy as np\r\n",
    "from sklearn.metrics import classification_report"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    " # Split dataset into Train, Validation & Test sets\r\n",
    "\r\n",
    "parent_folder = './data'\r\n",
    "size = 299   # Resize all images to (size,size)\r\n",
    "bs = 32      # Batch size\r\n",
    "\r\n",
    "\r\n",
    "# Data augmentation on train dataset only\r\n",
    "train_data_gen = ImageDataGenerator(width_shift_range = 0.1, height_shift_range = 0.1, zoom_range=0.1, shear_range=0.1, brightness_range=[0.8,1.2], validation_split=0.15, preprocessing_function=preprocess_input)\r\n",
    "train_data = train_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', batch_size=bs, seed=42, subset='training')\r\n",
    "\r\n",
    "validation_data_gen = ImageDataGenerator(validation_split=0.15, preprocessing_function=preprocess_input)\r\n",
    "validation_data = validation_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', batch_size=bs, seed=42, subset='validation')\r\n",
    "\r\n",
    "test_data_gen = ImageDataGenerator(validation_split=0.10, preprocessing_function=preprocess_input)\r\n",
    "test_data = test_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', subset='validation', shuffle=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 357 images belonging to 4 classes.\n",
      "Found 63 images belonging to 4 classes.\n",
      "Found 42 images belonging to 4 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Assign essential variables\r\n",
    "shape = train_data.image_shape                 # Shape of train images (height,width,channels)\r\n",
    "print(shape)\r\n",
    "k = train_data.num_classes                     # Total number of labels or classes\r\n",
    "train_samples = train_data.samples             # Total number of images in train set\r\n",
    "validation_samples = validation_data.samples   # total number of images in validation set"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(299, 299, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Build the model\r\n",
    "\r\n",
    "input = Input(shape=shape)\r\n",
    " \r\n",
    "basemodel = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=shape, pooling='avg')   # Basemodel is InceptionV3 with pretrained weights trained on imagenet dataset\r\n",
    "basemodel.trainable = False                                                                        # Freeze the weights in all layers of the CNN\r\n",
    " \r\n",
    "x = basemodel(input)\r\n",
    "x = Dense(1024, activation='relu')(x)\r\n",
    "x = Dropout(0.2)(x)\r\n",
    "output = Dense(k, activation='softmax')(x)\r\n",
    " \r\n",
    "model = Model(input,output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Compile the model\r\n",
    "\r\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Initialize callbacks\r\n",
    " \r\n",
    "stop = EarlyStopping(monitor='val_loss', patience=4, mode='min', restore_best_weights=True)                                             # Stops training early to prevent overfitting\r\n",
    "checkpoint = ModelCheckpoint(filepath='./weights/{val_loss:.4f}-weights-{epoch:02d}.hdf5', monitor='val_loss', mode='min', save_best_only=True)   # Saves the model for every best val_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Train the model\r\n",
    "\r\n",
    "ep = 50                      # Number of epochs\r\n",
    "spe = train_samples/bs       # Steps per epoch\r\n",
    "vs = validation_samples/bs   # Validation steps\r\n",
    " \r\n",
    "r = model.fit(train_data, validation_data=validation_data, steps_per_epoch=spe, validation_steps=vs, epochs=ep, callbacks=[stop,checkpoint])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 41s 3s/step - loss: 0.3170 - accuracy: 0.8655 - val_loss: 0.0222 - val_accuracy: 0.9841\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 32s 3s/step - loss: 0.0502 - accuracy: 0.9860 - val_loss: 0.0448 - val_accuracy: 0.9841\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 33s 3s/step - loss: 0.0258 - accuracy: 0.9944 - val_loss: 0.0188 - val_accuracy: 0.9841\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 34s 3s/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 34s 3s/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 33s 3s/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 0.9841\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 33s 3s/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 33s 3s/step - loss: 0.0064 - accuracy: 0.9972 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 33s 3s/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 32s 3s/step - loss: 0.0278 - accuracy: 0.9888 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 33s 3s/step - loss: 0.0076 - accuracy: 0.9972 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 33s 3s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 34s 3s/step - loss: 3.4340e-04 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 34s 3s/step - loss: 8.4951e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 35s 3s/step - loss: 3.0119e-04 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Predictions on the test data\r\n",
    "\r\n",
    "pred = model.predict(test_data).argmax(axis=1)\r\n",
    "labels = list(train_data.class_indices.keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Get the F1 score on test data prediction\r\n",
    "\r\n",
    "print(classification_report(test_data.classes, pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00        12\n",
      "           3       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model.save('model/model_classification_20220116')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model/model_classification_20220116\\assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "img_path = 'triangle.png'\r\n",
    "img = image.load_img(img_path, target_size=(299, 299))\r\n",
    "x = image.img_to_array(img) \r\n",
    "x = np.expand_dims(x, axis=0)\r\n",
    "x = preprocess_input(x)\r\n",
    "\r\n",
    "preds = model.predict(x).argmax(axis=1)\r\n",
    "# decode the results into a list of tuples (class, description, probability)\r\n",
    "# (one such list for each sample in the batch)\r\n",
    "print('Predicted:', preds)\r\n",
    "# print(classification_report(test_data.classes, pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted: [3]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "from tensorflow_docs.vis import embed\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "import imageio"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "batch_size = 64\r\n",
    "num_channels = 1\r\n",
    "num_classes = 10\r\n",
    "image_size = 28\r\n",
    "latent_dim = 128"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# We'll use all the available examples from both the training and test\r\n",
    "# sets.\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n",
    "all_digits = np.concatenate([x_train, x_test])\r\n",
    "all_labels = np.concatenate([y_train, y_test])\r\n",
    "\r\n",
    "# Scale the pixel values to [0, 1] range, add a channel dimension to\r\n",
    "# the images, and one-hot encode the labels.\r\n",
    "all_digits = all_digits.astype(\"float32\") / 255.0\r\n",
    "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\r\n",
    "all_labels = keras.utils.to_categorical(all_labels, 10)\r\n",
    "\r\n",
    "# Create tf.data.Dataset.\r\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_digits, all_labels))\r\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\r\n",
    "\r\n",
    "print(f\"Shape of training images: {all_digits.shape}\")\r\n",
    "print(f\"Shape of training labels: {all_labels.shape}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n",
      "11501568/11490434 [==============================] - 2s 0us/step\n",
      "Shape of training images: (70000, 28, 28, 1)\n",
      "Shape of training labels: (70000, 10)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "generator_in_channels = latent_dim + num_classes\r\n",
    "discriminator_in_channels = num_channels + num_classes\r\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "138 11\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Create the discriminator.\r\n",
    "discriminator = keras.Sequential(\r\n",
    "    [\r\n",
    "        keras.layers.InputLayer((28, 28, discriminator_in_channels)),\r\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\r\n",
    "        layers.LeakyReLU(alpha=0.2),\r\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\r\n",
    "        layers.LeakyReLU(alpha=0.2),\r\n",
    "        layers.GlobalMaxPooling2D(),\r\n",
    "        layers.Dense(1),\r\n",
    "    ],\r\n",
    "    name=\"discriminator\",\r\n",
    ")\r\n",
    "\r\n",
    "# Create the generator.\r\n",
    "generator = keras.Sequential(\r\n",
    "    [\r\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\r\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\r\n",
    "        # 7x7x(128 + num_classes) map.\r\n",
    "        layers.Dense(7 * 7 * generator_in_channels),\r\n",
    "        layers.LeakyReLU(alpha=0.2),\r\n",
    "        layers.Reshape((7, 7, generator_in_channels)),\r\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\r\n",
    "        layers.LeakyReLU(alpha=0.2),\r\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\r\n",
    "        layers.LeakyReLU(alpha=0.2),\r\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\r\n",
    "    ],\r\n",
    "    name=\"generator\",\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class ConditionalGAN(keras.Model):\r\n",
    "    def __init__(self, discriminator, generator, latent_dim):\r\n",
    "        super(ConditionalGAN, self).__init__()\r\n",
    "        self.discriminator = discriminator\r\n",
    "        self.generator = generator\r\n",
    "        self.latent_dim = latent_dim\r\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\r\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\r\n",
    "\r\n",
    "    @property\r\n",
    "    def metrics(self):\r\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\r\n",
    "\r\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\r\n",
    "        super(ConditionalGAN, self).compile()\r\n",
    "        self.d_optimizer = d_optimizer\r\n",
    "        self.g_optimizer = g_optimizer\r\n",
    "        self.loss_fn = loss_fn\r\n",
    "\r\n",
    "    def train_step(self, data):\r\n",
    "        # Unpack the data.\r\n",
    "        real_images, one_hot_labels = data\r\n",
    "\r\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\r\n",
    "        # the images. This is for the discriminator.\r\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\r\n",
    "        image_one_hot_labels = tf.repeat(\r\n",
    "            image_one_hot_labels, repeats=[image_size * image_size]\r\n",
    "        )\r\n",
    "        image_one_hot_labels = tf.reshape(\r\n",
    "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\r\n",
    "        )\r\n",
    "\r\n",
    "        # Sample random points in the latent space and concatenate the labels.\r\n",
    "        # This is for the generator.\r\n",
    "        batch_size = tf.shape(real_images)[0]\r\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\r\n",
    "        random_vector_labels = tf.concat(\r\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\r\n",
    "        )\r\n",
    "\r\n",
    "        # Decode the noise (guided by labels) to fake images.\r\n",
    "        generated_images = self.generator(random_vector_labels)\r\n",
    "\r\n",
    "        # Combine them with real images. Note that we are concatenating the labels\r\n",
    "        # with these images here.\r\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\r\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\r\n",
    "        combined_images = tf.concat(\r\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\r\n",
    "        )\r\n",
    "\r\n",
    "        # Assemble labels discriminating real from fake images.\r\n",
    "        labels = tf.concat(\r\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\r\n",
    "        )\r\n",
    "\r\n",
    "        # Train the discriminator.\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            predictions = self.discriminator(combined_images)\r\n",
    "            d_loss = self.loss_fn(labels, predictions)\r\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\r\n",
    "        self.d_optimizer.apply_gradients(\r\n",
    "            zip(grads, self.discriminator.trainable_weights)\r\n",
    "        )\r\n",
    "\r\n",
    "        # Sample random points in the latent space.\r\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\r\n",
    "        random_vector_labels = tf.concat(\r\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\r\n",
    "        )\r\n",
    "\r\n",
    "        # Assemble labels that say \"all real images\".\r\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\r\n",
    "\r\n",
    "        # Train the generator (note that we should *not* update the weights\r\n",
    "        # of the discriminator)!\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            fake_images = self.generator(random_vector_labels)\r\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\r\n",
    "            predictions = self.discriminator(fake_image_and_labels)\r\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\r\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\r\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\r\n",
    "\r\n",
    "        # Monitor loss.\r\n",
    "        self.gen_loss_tracker.update_state(g_loss)\r\n",
    "        self.disc_loss_tracker.update_state(d_loss)\r\n",
    "        return {\r\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\r\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\r\n",
    "        }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "cond_gan = ConditionalGAN(\r\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\r\n",
    ")\r\n",
    "cond_gan.compile(\r\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\r\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\r\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\r\n",
    ")\r\n",
    "\r\n",
    "cond_gan.fit(dataset, epochs=20)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1094/1094 [==============================] - 551s 503ms/step - g_loss: 1.5720 - d_loss: 0.4245\n",
      "Epoch 2/20\n",
      "1094/1094 [==============================] - 550s 503ms/step - g_loss: 1.4651 - d_loss: 0.4489\n",
      "Epoch 3/20\n",
      "1094/1094 [==============================] - 551s 503ms/step - g_loss: 1.7205 - d_loss: 0.3732\n",
      "Epoch 4/20\n",
      "1094/1094 [==============================] - 551s 504ms/step - g_loss: 2.2214 - d_loss: 0.2360\n",
      "Epoch 5/20\n",
      "1094/1094 [==============================] - 550s 503ms/step - g_loss: 2.8402 - d_loss: 0.1658\n",
      "Epoch 6/20\n",
      "1094/1094 [==============================] - 550s 503ms/step - g_loss: 1.1973 - d_loss: 0.5881\n",
      "Epoch 7/20\n",
      "1094/1094 [==============================] - 550s 503ms/step - g_loss: 1.0869 - d_loss: 0.6220\n",
      "Epoch 8/20\n",
      "1094/1094 [==============================] - 550s 502ms/step - g_loss: 0.9313 - d_loss: 0.6462\n",
      "Epoch 9/20\n",
      "1094/1094 [==============================] - 549s 502ms/step - g_loss: 0.8627 - d_loss: 0.6691\n",
      "Epoch 10/20\n",
      "1094/1094 [==============================] - 550s 502ms/step - g_loss: 0.8500 - d_loss: 0.6663\n",
      "Epoch 11/20\n",
      "1094/1094 [==============================] - 534s 488ms/step - g_loss: 0.7962 - d_loss: 0.6787\n",
      "Epoch 12/20\n",
      "1094/1094 [==============================] - 526s 481ms/step - g_loss: 0.7860 - d_loss: 0.6894\n",
      "Epoch 13/20\n",
      " 438/1094 [===========>..................] - ETA: 5:18 - g_loss: 0.7778 - d_loss: 0.6844"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13148/662719055.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcond_gan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python\\iwannabewizard\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\iwannabewizard\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\iwannabewizard\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\iwannabewizard\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\iwannabewizard\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\iwannabewizard\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\iwannabewizard\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\python\\iwannabewizard\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\iwannabewizard\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We first extract the trained generator from our Conditiona GAN.\r\n",
    "trained_gen = cond_gan.generator\r\n",
    "\r\n",
    "# Choose the number of intermediate images that would be generated in\r\n",
    "# between the interpolation + 2 (start and last images).\r\n",
    "num_interpolation = 9  # @param {type:\"integer\"}\r\n",
    "\r\n",
    "# Sample noise for the interpolation.\r\n",
    "interpolation_noise = tf.random.normal(shape=(1, latent_dim))\r\n",
    "interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)\r\n",
    "interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))\r\n",
    "\r\n",
    "\r\n",
    "def interpolate_class(first_number, second_number):\r\n",
    "    # Convert the start and end labels to one-hot encoded vectors.\r\n",
    "    first_label = keras.utils.to_categorical([first_number], num_classes)\r\n",
    "    second_label = keras.utils.to_categorical([second_number], num_classes)\r\n",
    "    first_label = tf.cast(first_label, tf.float32)\r\n",
    "    second_label = tf.cast(second_label, tf.float32)\r\n",
    "\r\n",
    "    # Calculate the interpolation vector between the two labels.\r\n",
    "    percent_second_label = tf.linspace(0, 1, num_interpolation)[:, None]\r\n",
    "    percent_second_label = tf.cast(percent_second_label, tf.float32)\r\n",
    "    interpolation_labels = (\r\n",
    "        first_label * (1 - percent_second_label) + second_label * percent_second_label\r\n",
    "    )\r\n",
    "\r\n",
    "    # Combine the noise and the labels and run inference with the generator.\r\n",
    "    noise_and_labels = tf.concat([interpolation_noise, interpolation_labels], 1)\r\n",
    "    fake = trained_gen.predict(noise_and_labels)\r\n",
    "    return fake\r\n",
    "\r\n",
    "\r\n",
    "start_class = 1  # @param {type:\"slider\", min:0, max:9, step:1}\r\n",
    "end_class = 5  # @param {type:\"slider\", min:0, max:9, step:1}\r\n",
    "\r\n",
    "fake_images = interpolate_class(start_class, end_class)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fake_images *= 255.0\r\n",
    "converted_images = fake_images.astype(np.uint8)\r\n",
    "converted_images = tf.image.resize(converted_images, (96, 96)).numpy().astype(np.uint8)\r\n",
    "imageio.mimsave(\"animation.gif\", converted_images, fps=1)\r\n",
    "embed.embed_file(\"animation.gif\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.9 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "interpreter": {
   "hash": "aa191c739e4238b6bce09f0aa2a4d176429cce0a8f0c895412859a226c16b236"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}